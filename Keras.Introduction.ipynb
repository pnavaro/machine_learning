{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD : `keras` & perceptron multi-couches\n",
    "\n",
    "Romain Tavenard\n",
    "Creative Commons CC BY-NC-SA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette séance, nous nous focaliserons sur la création et l'étude de modèles\n",
    "de type perceptron multi-couches à l'aide de la librairie `keras`.\n",
    "\n",
    "Pour cela, vous utiliserez la classe de modèles `Sequential()` de `keras`.\n",
    "Voici ci-dessous un exemple de définition d'un tel modèle :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: KERAS_BACKEND=theano\n"
     ]
    }
   ],
   "source": [
    "%env KERAS_BACKEND=theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.datasets import mnist, boston_housing\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. définir les couches et les ajouter l'une après l'autre au modèle\n",
    "premiere_couche = Dense(units=12, activation=\"relu\", input_dim=24)\n",
    "couche_cachee = Dense(units=12, activation=\"sigmoid\")\n",
    "couche_sortie = Dense(units=3, activation=\"linear\")\n",
    "\n",
    "model = Sequential()\n",
    "model.add(premiere_couche)\n",
    "model.add(couche_cachee)\n",
    "model.add(couche_sortie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Spécifier l'algo d'optimisation et la fonction de risque à optimiser\n",
    "\n",
    "Fonctions de risque classiques :\n",
    " * \"mse\" en régression,\n",
    " * \"categorical_crossentropy\" en classification multi-classes\n",
    " * \"binary_crossentropy\" en classification binaire\n",
    " \n",
    " On peut y ajouter des métriques supplémentaires (ici taux de bonne\n",
    " classifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"sgd\", loss=\"mse\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Lancer l'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-08e965269dbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "#model.fit(X_train, y_train, verbose=2, epochs=10, batch_size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Préparation des données\n",
    "\n",
    "Pour ce TD, nous vous proposons d'utiliser les fonctions suivantes pour préparer\n",
    "vos données à l'utilisation dans `keras` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.datasets import mnist, boston_housing\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_mnist():\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = x_train.reshape((x_train.shape[0], -1))\n",
    "    x_test = x_test.reshape((x_test.shape[0], -1))\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_boston():\n",
    "    (x_train, y_train), (x_test, y_test) = boston_housing.load_data()\n",
    "    \n",
    "    scaler_x = MinMaxScaler()\n",
    "    scaler_x.fit(x_train)\n",
    "    x_train = scaler_x.transform(x_train)\n",
    "    x_test = scaler_x.transform(x_test)\n",
    "    scaler_y = MinMaxScaler()\n",
    "    scaler_y.fit(y_train[:,np.newaxis])\n",
    "    y_train = scaler_y.transform(y_train[:,np.newaxis])\n",
    "    y_test = scaler_y.transform(y_test[:,np.newaxis])\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/keras-datasets/boston_housing.npz\n",
      "57344/57026 [==============================] - 0s 5us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((404, 13), (404, 1))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = prepare_boston()\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((404, 13), (404, 1))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Observez le code des fonctions `prepare_mnist` et `prepare_boston`.\n",
    "Que font ces fonctions ? Quelles sont les dimensions des matrices / vecteurs à\n",
    "la sortie ? S'agit-il de problèmes de classification ou de régression ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Premiers réseaux de neurone\n",
    "\n",
    "2. Chargez le jeu de données MNIST et apprenez un premier modèle sans couche\n",
    "cachée avec une fonction d'activation raisonnable pour les neurones de la couche\n",
    "de sortie. Pour cette question comme pour les suivantes, limitez vous à un\n",
    "nombre d'itérations de l'ordre de 10 : ce n'est absolument pas réaliste, mais\n",
    "cela vous évitera de perdre trop de temps à scruter l'apprentissage de vos\n",
    "modèles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 3s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/tensorflow/lib/python3.5/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype uint8 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((60000, 784), (60000, 10))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# regression logicstique\n",
    "x_train, x_test, y_train, y_test = prepare_mnist()\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 3s - loss: 0.8713 - acc: 0.7866\n",
      "Epoch 2/10\n",
      " - 1s - loss: 0.4309 - acc: 0.8887\n",
      "Epoch 3/10\n",
      " - 1s - loss: 0.3635 - acc: 0.9024\n",
      "Epoch 4/10\n",
      " - 1s - loss: 0.3324 - acc: 0.9096\n",
      "Epoch 5/10\n",
      " - 1s - loss: 0.3138 - acc: 0.9135\n",
      "Epoch 6/10\n",
      " - 1s - loss: 0.3016 - acc: 0.9171\n",
      "Epoch 7/10\n",
      " - 1s - loss: 0.2928 - acc: 0.9182\n",
      "Epoch 8/10\n",
      " - 1s - loss: 0.2859 - acc: 0.9203\n",
      "Epoch 9/10\n",
      " - 1s - loss: 0.2806 - acc: 0.9215\n",
      "Epoch 10/10\n",
      " - 1s - loss: 0.2758 - acc: 0.9231\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f772eb32710>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_layer = Dense(units=y_train.shape[1], \n",
    "                    activation=\"softmax\", \n",
    "                    input_dim=x_train.shape[1])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(input_layer)\n",
    "model.compile(optimizer=\"adam\",\n",
    "             loss=\"categorical_crossentropy\",\n",
    "             metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=256, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Comparez les performances de ce premier modèle è celle de modèles avec\n",
    "respectivement 1, 2 et 3 couches cachées de 128 neurones chacune. Vous\n",
    "utiliserez la fonction ReLU (`\"relu\"`) comme fonction d'activation pour les\n",
    "neurones des couches cachées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 1s - loss: 0.4186 - acc: 0.8809\n",
      "Epoch 2/10\n",
      " - 1s - loss: 0.1586 - acc: 0.9540\n",
      "Epoch 3/10\n",
      " - 1s - loss: 0.1128 - acc: 0.9668\n",
      "Epoch 4/10\n",
      " - 1s - loss: 0.0869 - acc: 0.9747\n",
      "Epoch 5/10\n",
      " - 1s - loss: 0.0678 - acc: 0.9793\n",
      "Epoch 6/10\n",
      " - 1s - loss: 0.0550 - acc: 0.9836\n",
      "Epoch 7/10\n",
      " - 1s - loss: 0.0458 - acc: 0.9864\n",
      "Epoch 8/10\n",
      " - 1s - loss: 0.0376 - acc: 0.9892\n",
      "Epoch 9/10\n",
      " - 1s - loss: 0.0325 - acc: 0.9905\n",
      "Epoch 10/10\n",
      " - 1s - loss: 0.0266 - acc: 0.9926\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7727b70a58>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_layer = Dense(units=128, \n",
    "                    activation=\"relu\", \n",
    "                    input_dim=x_train.shape[1])\n",
    "\n",
    "second_layer = Dense(units=128, \n",
    "                    activation=\"relu\", \n",
    "                    input_dim=x_train.shape[1])\n",
    "\n",
    "\n",
    "third_layer = Dense(units=y_train.shape[1], \n",
    "                    activation=\"softmax\")\n",
    "\n",
    "model = Sequential()\n",
    "model.add(input_layer)\n",
    "model.add(second_layer)\n",
    "model.add(third_layer)\n",
    "\n",
    "model.compile(optimizer=\"adam\",\n",
    "             loss=\"categorical_crossentropy\",\n",
    "             metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=256, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. On peut obtenir le nombre de paramètres d'un modèle à l'aide de la\n",
    "méthode `count_params()`. Comptez ainsi le nombre de paramètres du modèle à\n",
    "3 couches cachées et définissez un modèle à une seule couche cachée ayant un\n",
    "nombre comparable de paramètres. Parmi ces deux modèles, lequel semble le plus\n",
    "performant ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      " - 1s - loss: 0.4320 - acc: 0.8811 - val_loss: 0.1639 - val_acc: 0.9570\n",
      "Epoch 2/10\n",
      " - 1s - loss: 0.1713 - acc: 0.9509 - val_loss: 0.1170 - val_acc: 0.9665\n",
      "Epoch 3/10\n",
      " - 1s - loss: 0.1187 - acc: 0.9644 - val_loss: 0.0963 - val_acc: 0.9722\n",
      "Epoch 4/10\n",
      " - 1s - loss: 0.0892 - acc: 0.9735 - val_loss: 0.0958 - val_acc: 0.9738\n",
      "Epoch 5/10\n",
      " - 1s - loss: 0.0714 - acc: 0.9783 - val_loss: 0.0814 - val_acc: 0.9765\n",
      "Epoch 6/10\n",
      " - 1s - loss: 0.0577 - acc: 0.9827 - val_loss: 0.0822 - val_acc: 0.9760\n",
      "Epoch 7/10\n",
      " - 1s - loss: 0.0482 - acc: 0.9858 - val_loss: 0.0740 - val_acc: 0.9785\n",
      "Epoch 8/10\n",
      " - 1s - loss: 0.0405 - acc: 0.9880 - val_loss: 0.0713 - val_acc: 0.9808\n",
      "Epoch 9/10\n",
      " - 1s - loss: 0.0315 - acc: 0.9909 - val_loss: 0.0698 - val_acc: 0.9792\n",
      "Epoch 10/10\n",
      " - 1s - loss: 0.0262 - acc: 0.9930 - val_loss: 0.0782 - val_acc: 0.9783\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'confusion_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-37eeb284ab8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'confusion_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "input_layer = Dense(units=128, \n",
    "                    activation=\"relu\", \n",
    "                    input_dim=x_train.shape[1])\n",
    "\n",
    "second_layer = Dense(units=128, \n",
    "                    activation=\"relu\", \n",
    "                    input_dim=x_train.shape[1])\n",
    "\n",
    "\n",
    "third_layer = Dense(units=y_train.shape[1], \n",
    "                    activation=\"softmax\")\n",
    "\n",
    "model = Sequential()\n",
    "model.add(input_layer)\n",
    "model.add(second_layer)\n",
    "model.add(third_layer)\n",
    "\n",
    "model.compile(optimizer=\"adam\",\n",
    "             loss=\"categorical_crossentropy\",\n",
    "             metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, \n",
    "          epochs=10, batch_size=256, \n",
    "          verbose=2, validation_split=0.1)\n",
    "\n",
    "y_pred = model.predict(x_train)\n",
    "\n",
    "confusion_matrix(y_train.argmax(axis=1), y_pred.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118282"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.count_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilisation d'un jeu de validation\n",
    "\n",
    "Bien entendu, les observations faites plus haut ne sont pas suffisantes,\n",
    "notamment parce qu'elles ne permettent pas de se rendre compte de l'ampleur du\n",
    "phénomène de sur-apprentissage.\n",
    "\n",
    "Pour y remédier, `keras` permet de fixer, lors de l'appel à la méthode `fit()`,\n",
    "une fraction du jeu d'apprentissage à utiliser pour la validation.\n",
    "Jetez un oeil\n",
    "[ici](https://keras.io/getting-started/faq/#how-is-the-validation-split-computed)\n",
    "pour comprendre comment les exemples de validation sont\n",
    "choisis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Répétez les comparaisons de modèles ci-dessus en vous focalisant sur le taux\n",
    "de bonnes classifications obtenu sur le jeu de validation (vous prendrez 30\\%\n",
    "    du jeu d'apprentissage pour votre validation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Régularisation et _Drop-Out_\n",
    "\n",
    "6. Appliquez une régularisation de type $L_1$ à chacune des couches de votre\n",
    "réseau. L'aide disponible [ici](https://keras.io/regularizers/) devrait\n",
    "vous aider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Au lieu de la régularisation $L_1$, choisissez de mettre en place une\n",
    "stratégie de [_Drop-Out_](https://keras.io/layers/core/#dropout) pour aider à la\n",
    "régularisation de votre réseau.\n",
    "Vous éteindrez à chaque étape 10\\% des poids de votre réseau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithme d'optimisation et vitesse de convergence\n",
    "\n",
    "8. Modifiez la méthode d'optimisation choisie. Vous pourrez notamment essayer\n",
    "les algorithmes `\"rmsprop\"` et `\"adam\"`, reconnus pour leurs performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. En utilisant l'aide fournie [ici](https://keras.io/optimizers/), faites\n",
    "varier le paramètre `lr` (_learning rate_) à l'extrême pour observer :\n",
    "\n",
    "* l'instabilité des performances lorsque celui-ci est trop grand ;\n",
    "* la lenteur de la convergence lorsque celui-ci est trop petit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèles `keras` dans `sklearn`\n",
    "\n",
    "Il est possible de transformer vos modèles `keras` (en tout cas, ceux qui sont\n",
    "    de type `Sequential`) en modèles `sklearn`. Cela a notamment pour avantage\n",
    "de vous permettre d'utiliser les fonctionnalités de sélection de modèles vues\n",
    "lors du TD précédent.\n",
    "\n",
    "Pour cela, vous devrez utiliser au choix l'une des classes `KerasClassifier` ou\n",
    "`KerasRegressor` (selon le problème de _machine learning_ auquel vous êtes\n",
    "    confronté) du module `keras.wrappers.scikit-learn`.\n",
    "\n",
    "Le principe de fonctionnement de ces deux classes est le même :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/tensorflow/lib/python3.5/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype uint8 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 1s - loss: 1.9457 - acc: 0.4734\n",
      "Epoch 2/10\n",
      " - 0s - loss: 1.1317 - acc: 0.7762\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.7130 - acc: 0.8371\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.5584 - acc: 0.8597\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.4829 - acc: 0.8737\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.4377 - acc: 0.8828\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.4069 - acc: 0.8885\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.3841 - acc: 0.8929\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.3665 - acc: 0.8975\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.3522 - acc: 0.9009\n",
      "Epoch 1/10\n",
      " - 1s - loss: 1.9436 - acc: 0.4623\n",
      "Epoch 2/10\n",
      " - 0s - loss: 1.1429 - acc: 0.7621\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.7295 - acc: 0.8271\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.5688 - acc: 0.8550\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.4876 - acc: 0.8706\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.4378 - acc: 0.8807\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.4043 - acc: 0.8892\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.3800 - acc: 0.8946\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.3612 - acc: 0.8990\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.3460 - acc: 0.9024\n",
      "Epoch 1/10\n",
      " - 1s - loss: 1.9644 - acc: 0.4688\n",
      "Epoch 2/10\n",
      " - 0s - loss: 1.1468 - acc: 0.7617\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.7223 - acc: 0.8309\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.5620 - acc: 0.8581\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.4824 - acc: 0.8724\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.4350 - acc: 0.8819\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.4029 - acc: 0.8900\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.3796 - acc: 0.8951\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.3617 - acc: 0.8997\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.3470 - acc: 0.9029\n",
      "Epoch 1/10\n",
      " - 1s - loss: 0.4884 - acc: 0.8660\n",
      "Epoch 2/10\n",
      " - 1s - loss: 0.1892 - acc: 0.9459\n",
      "Epoch 3/10\n",
      " - 1s - loss: 0.1335 - acc: 0.9612\n",
      "Epoch 4/10\n",
      " - 1s - loss: 0.1026 - acc: 0.9700\n",
      "Epoch 5/10\n",
      " - 1s - loss: 0.0807 - acc: 0.9762\n",
      "Epoch 6/10\n",
      " - 1s - loss: 0.0650 - acc: 0.9813\n",
      "Epoch 7/10\n",
      " - 1s - loss: 0.0550 - acc: 0.9842\n",
      "Epoch 8/10\n",
      " - 1s - loss: 0.0462 - acc: 0.9859\n",
      "Epoch 9/10\n",
      " - 1s - loss: 0.0378 - acc: 0.9887\n",
      "Epoch 10/10\n",
      " - 1s - loss: 0.0320 - acc: 0.9909\n",
      "Epoch 1/10\n",
      " - 1s - loss: 0.4980 - acc: 0.8656\n",
      "Epoch 2/10\n",
      " - 1s - loss: 0.1909 - acc: 0.9457\n",
      "Epoch 3/10\n",
      " - 1s - loss: 0.1385 - acc: 0.9599\n",
      "Epoch 4/10\n",
      " - 1s - loss: 0.1076 - acc: 0.9682\n",
      "Epoch 5/10\n",
      " - 1s - loss: 0.0893 - acc: 0.9741\n",
      "Epoch 6/10\n",
      " - 1s - loss: 0.0699 - acc: 0.9791\n",
      "Epoch 7/10\n",
      " - 1s - loss: 0.0570 - acc: 0.9831\n",
      "Epoch 8/10\n",
      " - 1s - loss: 0.0494 - acc: 0.9852\n",
      "Epoch 9/10\n",
      " - 1s - loss: 0.0394 - acc: 0.9885\n",
      "Epoch 10/10\n",
      " - 1s - loss: 0.0336 - acc: 0.9902\n",
      "Epoch 1/10\n",
      " - 1s - loss: 0.4978 - acc: 0.8615\n",
      "Epoch 2/10\n",
      " - 1s - loss: 0.1904 - acc: 0.9449\n",
      "Epoch 3/10\n",
      " - 1s - loss: 0.1339 - acc: 0.9606\n",
      "Epoch 4/10\n",
      " - 1s - loss: 0.1037 - acc: 0.9689\n",
      "Epoch 5/10\n",
      " - 1s - loss: 0.0813 - acc: 0.9760\n",
      "Epoch 6/10\n",
      " - 1s - loss: 0.0661 - acc: 0.9806\n",
      "Epoch 7/10\n",
      " - 1s - loss: 0.0526 - acc: 0.9851\n",
      "Epoch 8/10\n",
      " - 1s - loss: 0.0438 - acc: 0.9873\n",
      "Epoch 9/10\n",
      " - 1s - loss: 0.0388 - acc: 0.9890\n",
      "Epoch 10/10\n",
      " - 1s - loss: 0.0310 - acc: 0.9920\n",
      "Epoch 1/10\n",
      " - 1s - loss: 0.4333 - acc: 0.8780\n",
      "Epoch 2/10\n",
      " - 1s - loss: 0.1930 - acc: 0.9425\n",
      "Epoch 3/10\n",
      " - 1s - loss: 0.1358 - acc: 0.9593\n",
      "Epoch 4/10\n",
      " - 1s - loss: 0.1045 - acc: 0.9677\n",
      "Epoch 5/10\n",
      " - 1s - loss: 0.0846 - acc: 0.9741\n",
      "Epoch 6/10\n",
      " - 1s - loss: 0.0678 - acc: 0.9795\n",
      "Epoch 7/10\n",
      " - 1s - loss: 0.0557 - acc: 0.9829\n",
      "Epoch 8/10\n",
      " - 1s - loss: 0.0464 - acc: 0.9852\n",
      "Epoch 9/10\n",
      " - 1s - loss: 0.0392 - acc: 0.9881\n",
      "Epoch 10/10\n",
      " - 1s - loss: 0.0321 - acc: 0.9901\n",
      "Epoch 1/10\n",
      " - 1s - loss: 0.4533 - acc: 0.8729\n",
      "Epoch 2/10\n",
      " - 1s - loss: 0.1978 - acc: 0.9417\n",
      "Epoch 3/10\n",
      " - 1s - loss: 0.1411 - acc: 0.9584\n",
      "Epoch 4/10\n",
      " - 1s - loss: 0.1093 - acc: 0.9674\n",
      "Epoch 5/10\n",
      " - 1s - loss: 0.0858 - acc: 0.9742\n",
      "Epoch 6/10\n",
      " - 1s - loss: 0.0710 - acc: 0.9777\n",
      "Epoch 7/10\n",
      " - 1s - loss: 0.0562 - acc: 0.9828\n",
      "Epoch 8/10\n",
      " - 1s - loss: 0.0470 - acc: 0.9858\n",
      "Epoch 9/10\n",
      " - 1s - loss: 0.0394 - acc: 0.9872\n",
      "Epoch 10/10\n",
      " - 1s - loss: 0.0307 - acc: 0.9909\n",
      "Epoch 1/10\n",
      " - 1s - loss: 0.4350 - acc: 0.8778\n",
      "Epoch 2/10\n",
      " - 1s - loss: 0.1945 - acc: 0.9425\n",
      "Epoch 3/10\n",
      " - 1s - loss: 0.1365 - acc: 0.9589\n",
      "Epoch 4/10\n",
      " - 1s - loss: 0.1044 - acc: 0.9683\n",
      "Epoch 5/10\n",
      " - 1s - loss: 0.0835 - acc: 0.9745\n",
      "Epoch 6/10\n",
      " - 1s - loss: 0.0672 - acc: 0.9798\n",
      "Epoch 7/10\n",
      " - 1s - loss: 0.0542 - acc: 0.9834\n",
      "Epoch 8/10\n",
      " - 1s - loss: 0.0453 - acc: 0.9863\n",
      "Epoch 9/10\n",
      " - 1s - loss: 0.0381 - acc: 0.9884\n",
      "Epoch 10/10\n",
      " - 1s - loss: 0.0310 - acc: 0.9903\n",
      "Epoch 1/10\n",
      " - 2s - loss: 0.4024 - acc: 0.8845\n",
      "Epoch 2/10\n",
      " - 1s - loss: 0.1553 - acc: 0.9554\n",
      "Epoch 3/10\n",
      " - 1s - loss: 0.1099 - acc: 0.9680\n",
      "Epoch 4/10\n",
      " - 1s - loss: 0.0837 - acc: 0.9757\n",
      "Epoch 5/10\n",
      " - 1s - loss: 0.0657 - acc: 0.9803\n",
      "Epoch 6/10\n",
      " - 1s - loss: 0.0536 - acc: 0.9842\n",
      "Epoch 7/10\n",
      " - 1s - loss: 0.0433 - acc: 0.9872\n",
      "Epoch 8/10\n",
      " - 1s - loss: 0.0369 - acc: 0.9893\n",
      "Epoch 9/10\n",
      " - 1s - loss: 0.0298 - acc: 0.9912\n",
      "Epoch 10/10\n",
      " - 1s - loss: 0.0255 - acc: 0.9929\n",
      "{'optimizer': 'adam'} 0.9705333333333334\n",
      "CPU times: user 2min 1s, sys: 10.7 s, total: 2min 12s\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.datasets import mnist, boston_housing\n",
    "from keras.utils import to_categorical\n",
    "import numpy\n",
    "\n",
    "\n",
    "def prepare_mnist():\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = x_train.reshape((x_train.shape[0], -1))\n",
    "    x_test = x_test.reshape((x_test.shape[0], -1))\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "def prepare_boston():\n",
    "    (x_train, y_train), (x_test, y_test) = boston_housing.load_data()\n",
    "    scaler_x = MinMaxScaler()\n",
    "    scaler_x.fit(x_train)\n",
    "    x_train = scaler_x.transform(x_train)\n",
    "    x_test = scaler_x.transform(x_test)\n",
    "    scaler_y = MinMaxScaler()\n",
    "    scaler_y.fit(y_train[:, numpy.newaxis])\n",
    "    y_train = scaler_y.transform(y_train[:, numpy.newaxis])\n",
    "    y_test = scaler_y.transform(y_test[:, numpy.newaxis])\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "def multi_layer_perceptron(input_dim, n_classes, activation=\"relu\", optimizer=\"sgd\"):\n",
    "    premiere_couche = Dense(units=128,\n",
    "                            activation=activation,\n",
    "                            input_dim=input_dim)\n",
    "    deuxieme_couche = Dense(units=128,\n",
    "                            activation=activation)\n",
    "    troisieme_couche = Dense(units=n_classes,\n",
    "                            activation=\"softmax\")\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(premiere_couche)\n",
    "    model.add(deuxieme_couche)\n",
    "    model.add(troisieme_couche)\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=\"categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = prepare_mnist()\n",
    "\n",
    "indices = numpy.random.permutation(X_train.shape[0])\n",
    "X_train = X_train[indices]\n",
    "y_train = y_train[indices]\n",
    "\n",
    "# premiere_couche = Dense(units=128,\n",
    "#                         activation=\"relu\",\n",
    "#                         input_dim=X_train.shape[1])\n",
    "# deuxieme_couche = Dense(units=128,\n",
    "#                         activation=\"relu\")\n",
    "# troisieme_couche = Dense(units=y_train.shape[1],\n",
    "#                         activation=\"softmax\")\n",
    "#\n",
    "# model = Sequential()\n",
    "# model.add(premiere_couche)\n",
    "# model.add(deuxieme_couche)\n",
    "# model.add(troisieme_couche)\n",
    "#\n",
    "# model.compile(optimizer=\"adam\",\n",
    "#               loss=\"categorical_crossentropy\",\n",
    "#               metrics=[\"accuracy\"])\n",
    "# model.fit(X_train, y_train, epochs=10, batch_size=256, verbose=2, validation_split=.1)\n",
    "#\n",
    "# y_pred = model.predict(X_train)\n",
    "# print(confusion_matrix(y_train.argmax(axis=1), y_pred.argmax(axis=1)))\n",
    "#\n",
    "# print(model.count_params())\n",
    "\n",
    "clf = KerasClassifier(build_fn=multi_layer_perceptron,\n",
    "                      input_dim=784,\n",
    "                      n_classes=10,\n",
    "                      activation=\"relu\",\n",
    "                      epochs=10,\n",
    "                      batch_size=256,\n",
    "                      verbose=2)\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "model_cv = GridSearchCV(estimator=clf,\n",
    "                        param_grid={\"optimizer\": [\"sgd\", \"adam\", \"rmsprop\"]},\n",
    "                        cv=KFold(n_splits=3))\n",
    "\n",
    "model_cv.fit(X_train, y_train)\n",
    "print(model_cv.best_params_, model_cv.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KerasClassifier(build_fn=multi_layer_perceptron,\n",
    "                      param_grid={\"optimizer\":}, \n",
    "                      param2=\"sgd\", ...)\n",
    "clf.fit(X, y)\n",
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois construit, l'objet `clf` s'utilise donc exactement comme un classifieur\n",
    "`sklearn`.\n",
    "L'attribut `build_fn` prend le nom d'une fonction qui retourne un modèle\n",
    "`keras`. Les autres paramètres passés lors de la construction du classifieur\n",
    "peuvent être :\n",
    "\n",
    "* des paramètres de votre fonction `ma_fonction` ;\n",
    "* des paramètres passés au modèle lors de son apprentissage (appel à la\n",
    "    méthode `fit()`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Créez un réseau à deux couches cachées transformé en objet `sklearn` en\n",
    "spécifiant, lors de sa construction, le nombre d'itérations et la taille des\n",
    "_batchs_ de votre descente de gradient par _mini-batchs_. Vous pourrez\n",
    "utiliser la méthode `score()` des objets `sklearn` pour évaluer ce modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Utilisez les outils de validation croisée de `sklearn` pour choisir entre\n",
    "les algorithmes d'optimisation `\"rmsprop\"` et `\"sgd\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# La notion de `Callback`\n",
    "\n",
    "Les _Callbacks_ sont des outils qui, dans `keras`, permettent d'avoir un oeil\n",
    "sur ce qui se passe lors de l'apprentissage et, éventuellement, d'agir sur cet\n",
    "apprentissage.\n",
    "\n",
    "Le premier _callback_ auquel vous pouvez accéder simplement est retourné\n",
    "lors de l'appel à la méthode `fit()` (sur un objet `keras`, pas `sklearn`). Ce\n",
    "_callback_ est un objet qui possède un attribut `history`. Cet attribut est un\n",
    "dictionnaire dont les clés sont les métriques suivies lors de l'apprentissage.\n",
    "À chacune de ces clés est associé un vecteur indiquant comment la quantité en\n",
    "question a évolué au fil des itérations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Tracez les courbes d'évolution du taux de bonnes classifications sur les\n",
    "jeux d'entrainement et de validation.\n",
    "\n",
    "La mise en place d'autres _callbacks_ doit être explicite. Elle se fait en\n",
    "passant une liste de _callbacks_ lors de l'appel à la méthode `fit()`.\n",
    "Lorsque l'apprentissage prend beaucoup de temps, la méthode précédente n'est pas\n",
    "satisfaisante car il est nécessaire d'attendre la fin du processus\n",
    "d'apprentissage avant de visualiser ces courbes. Dans ce cas, le _callback_\n",
    "[`TensorBoard`](https://keras.io/callbacks/#tensorboard) peut s'avérer très\n",
    "pratique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Visualisez dans une page TensorBoard l'évolution des métriques `\"loss\"`\n",
    "et `\"accuracy\"` lors de l'apprentissage d'un modèle.\n",
    "\n",
    "De même, lorsque l'apprentissage est long, il peut s'avérer souhaitable\n",
    "d'enregistrer des modèles intermédiaires, dans le cas où un plantage arriverait\n",
    "par exemple. Cela peut se faire à l'aide du _callback_\n",
    "[`ModelCheckpoint`](https://keras.io/callbacks/#modelcheckpoint)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Mettez en place un enregistrement des modèles intermédiaires toutes les 2\n",
    "itérations, en n'enregistrant un modèle que si le risque calculé sur le jeu de\n",
    "validation est plus faible que celui de tous les autres modèles enregistrés\n",
    "aux itérations précédentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. Mettez en oeuvre une politique d'arrêt précoce de l'apprentissage au cas où\n",
    "le risque calculé sur le jeu de validation n'a pas diminué depuis au moins 5\n",
    "itérations (en utilisant le _callback_\n",
    "[`EarlyStopping`](https://keras.io/callbacks/#earlystopping))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice de synthèse\n",
    "\n",
    "16. Mettez en place une validation croisée pour choisir la structure (nombre de\n",
    "    couches, nombre de neurones par couche) et l'algorithme d'optimisation\n",
    "    idoines pour le problème lié au jeu de données _Boston Housing_ (pour lequel\n",
    "        une fonction de préparation des données est fournie dans le module\n",
    "        `dataset_utils`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = prepare_mnist()\n",
    "\n",
    "indices = numpy.random.permutation(X_train.shape[0])\n",
    "X_train = X_train[indices]\n",
    "y_train = y_train[indices]\n",
    "\n",
    "# premiere_couche = Dense(units=128,\n",
    "#                         activation=\"relu\",\n",
    "#                         input_dim=X_train.shape[1])\n",
    "# deuxieme_couche = Dense(units=128,\n",
    "#                         activation=\"relu\")\n",
    "# troisieme_couche = Dense(units=y_train.shape[1],\n",
    "#                         activation=\"softmax\")\n",
    "#\n",
    "# model = Sequential()\n",
    "# model.add(premiere_couche)\n",
    "# model.add(deuxieme_couche)\n",
    "# model.add(troisieme_couche)\n",
    "#\n",
    "# model.compile(optimizer=\"adam\",\n",
    "#               loss=\"categorical_crossentropy\",\n",
    "#               metrics=[\"accuracy\"])\n",
    "# model.fit(X_train, y_train, epochs=10, batch_size=256, verbose=2, validation_split=.1)\n",
    "#\n",
    "# y_pred = model.predict(X_train)\n",
    "# print(confusion_matrix(y_train.argmax(axis=1), y_pred.argmax(axis=1)))\n",
    "#\n",
    "# print(model.count_params())\n",
    "\n",
    "clf = KerasClassifier(build_fn=multi_layer_perceptron,\n",
    "                      input_dim=784,\n",
    "                      n_classes=10,\n",
    "                      activation=\"relu\",\n",
    "                      epochs=10,\n",
    "                      batch_size=256,\n",
    "                      verbose=2)\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "model_cv = GridSearchCV(estimator=clf,\n",
    "                        param_grid={\"optimizer\": [\"sgd\", \"adam\", \"rmsprop\"]},\n",
    "                        cv=KFold(n_splits=3))\n",
    "\n",
    "model_cv.fit(X_train, y_train)\n",
    "print(model_cv.best_params_, model_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
